\section{Matrix Decompositions}

\def\moo#1{\emph{#1}}
\begin{slide}[LU Decomposition]
   $$PA =LU$$
   \begin{description}
     \item [Applicable to] Square matrices. Can also be used for rectangular ones, but rarely done so in practice
   \item[How]: Works by introducing $0$s below the $k^{th}$ diagonal in $k^{th}$ step. Let $x_k$ denote the $k{th}$ column.
   \item[Cost] $2n^3/3$ FLOPS
	\begin{multicols}{2}
	\begin{align*}
	L_k &= I-l_ke_k^T
	\\
	L_k^{-1} &= I+l_ke_k^T
	\\
	A_k &= L_kA_{k-1}
	\\
	L &= L_1^{-1}L_2^{-1}\cdots L_m^{-1}
	\end{align*}
	$$l_k = \left[ 
	\begin{array}{c}
	0
	\\
	\cfrac{x_{k+1,k}}{ x_{k,k}} 
	\\
	\vdots 
	\\
	\cfrac{x_{n,k}}{x_{k,k}}
	\end{array}
	\right]^T	
	\quad
	e_k = \left[ 
	\begin{array}{c}
	0
	\\
	\vdots
	\\
	0
	\\
	1
	\\
	\vdots 
	\\
	0
	\end{array}
	\right]^T
	$$
\end{multicols}
   \end{description}
\end{slide}
\begin{slide}[ Cholesky Decomposition]
   $$A = LL^*$$
   \begin{description}
   \item[Applicable to]: Hermitian\footnote{$A=A^T$} positive definite matrices. 
   \item[Cost]: $n^3/3$ FLOPS. Half of LU
   \item[Comments]: $L$ is lower triangular matrix has real positive diagonal entries. Unique when $A$ is positive definite. Non unique when $A$ is semi definite. Mainly used to solve System of linear equations, linear least square equation($A^TA=LL^T$)
   \item[How] 
   $$
   \begin{bmatrix} 
   a & w^{*}\\
   w&K
   \end{bmatrix}
   =
   \begin{bmatrix} 
   \sqrt{a} & 0\\
   w/\sqrt{a}&I
   \end{bmatrix}
   \begin{bmatrix} 
   1 & 0\\
   0&K-ww^{*}/\sqrt{a}
   \end{bmatrix}
   \begin{bmatrix} 
   \sqrt{a} & w^{*}/\sqrt{a}\\
   0&I
   \end{bmatrix}
   $$
   \end{description}
\end{slide}
\begin{slide}[ QR decomposition]
   $$ A = Q R$$
   \begin{description}
   \item[Applicable to]: Any $m\times n$ matrix
   \item[How] Gram-Schmidt process, Householder's reflections
   \item[Comment] $Q_{m\times m}$ is orthogonal matrix\footnote{Orthogonal $\implies Q^TQ=I$}, $R$ is upper triangular. Often used to solve linear least square problems.
   \item[Cost]
   \end{description}
\end{slide}
\begin{slide}[ Eigen Decomposition]
   $A = VDV^{-1} $
   \moo{Applicable to}: Square matrix $A$ with distinct eigen vectors. not necessarily distinct eigen values.
   \moo{How}: 
   \moo{Cost}:
   \moo{Comment} Also called Spectral Decomposition. $D$ is diagonal matrix having eigen values of $A$. $V$ has columns with corrresponding eigen vectors. $V$ if exists, is orthogonal.
\end{slide}
\begin{slide}[ SVD]
   $ $
   \moo{Applicable to}:
   \moo{How}: 
   \moo{Cost}:
\end{slide}
\begin{slide}[ Polar decomposition]
   $ $
   \moo{Applicable to}:
   \moo{How}: 
   \moo{Cost}:
\end{slide}
\begin{slide}[ Schur Decomposition]
   $ A = UTU^{*}$
   \moo{Applicable to}: Square matrices
   \moo{How}: 
   \moo{Cost}:
   \moo{Comment} $U$ is unitary, $T$ is upper triangular
\end{slide}

\begin{slide}
  Also called spectral decomposition
  Applicable to: square matrix A with distinct eigenvectors (not necessarily distinct eigenvalues).
  Decomposition: $A=VDV^{-1}$, where D is a diagonal matrix formed from the eigenvalues of A, and the columns of V are the corresponding eigenvectors of A.
  Existence: An n-by-n matrix A always has n eigenvalues, which can be ordered (in more than one way) to form an n-by-n diagonal matrix D and a corresponding matrix of nonzero columns V that satisfies the eigenvalue equation AV=VD. If the n eigenvectors (not necessarily eigenvalues) are distinct (that is, none is equal to any of the others), then V is invertible, implying the decomposition $A=VDV^{-1}$.[2]
  Comment: One can always normalize the eigenvectors to have length one (see definition of the eigenvalue equation). If A is real-symmetric, V is always invertible and can be made to have normalized columns. Then the equation $VV^T=I$ holds, because each eigenvector is orthogonal to the other. Therefore, the decomposition (which always exists if A is real-symmetric) reads as: $A=VDV^T$
  Comment: The condition of having n distinct eigenvalues is sufficient but not necessary. The necessary and sufficient condition is for each eigenvalue to have geometric multiplicity equal to its algebraic multiplicity.
  Comment: The eigendecomposition is useful for understanding the solution of a system of linear ordinary differential equations or linear difference equations. For example, the difference equation $x_{t+1}=Ax_t$ starting from the initial condition $x_0=c$ is solved by $x_t = A^tc$, which is equivalent to $x_t = VD^tV^{-1}c$, where V and D are the matrices formed from the eigenvectors and eigenvalues of A. Since D is diagonal, raising it to power $D^t$, just involves raising each element on the diagonal to the power t. This is much easier to do and to understand than raising A to power t, since A is usually not diagonal.

\end{slide}
